{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_ensemble(e_prob,y_test):\n",
    "    print(\"------Ensemble 3 classifier-------\")\n",
    "    print(\"Test BS:\",round(brier_score_loss(y_test,e_prob),3))\n",
    "    print()\n",
    "    y_pre_ens=np.where(pd.Series(e_prob)>0.5,1,0)\n",
    "    print(\"Test Accuracy:\",round(accuracy_score(y_pre_ens,y_test),3))\n",
    "    print()\n",
    "    print(\"Test F1-score:\",f1_score(y_pre_ens,y_test))\n",
    "    \n",
    "def print_base_ensemble(probs, y_test):\n",
    "    ens_proba=(probs['lgr']+probs['rfc']+probs['xgb'])/3\n",
    "    print_ensemble(ens_proba,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bayesian_base(probs, y_test, f1_rfc, f1_xgb, f1_lgr, f1_svm, uniform_prior=False):\n",
    "    ## krev ensemble:\n",
    "    probs_lgr = probs['lgr']\n",
    "    probs_rfc = probs['rfc']\n",
    "    probs_xgb = probs['xgb']\n",
    "    probs_svm = probs['svm']\n",
    "\n",
    "    # Equal priors or based on validation scores\n",
    "\n",
    "    # all of these give the same result\n",
    "    raw_weights = [f1_rfc, f1_xgb, f1_lgr, f1_svm]  # Adjust these based on your model validation scores\n",
    "    if uniform_prior:\n",
    "        raw_weights = [1, 1, 1, 1]  \n",
    "    # raw_weights = [acc_rfc, acc_xgb, acc_lgr]  \n",
    "    # raw_weights = [bs_rfc, bs_xgb, bs_lgr]\n",
    "\n",
    "    weights = [weight / sum(raw_weights) for weight in raw_weights]\n",
    "\n",
    "    bayesian_ensemble_proba = weights[0] * probs_rfc + weights[1] * probs_xgb + weights[2] * probs_lgr + weights[3] * probs_svm \n",
    "    print(\"Naive Bayes Ensemble Result\")\n",
    "    print_ensemble(bayesian_ensemble_proba,y_test)\n",
    "\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # Priors for model weights\n",
    "        weights = pm.Dirichlet('weights', a=np.array(raw_weights))  \n",
    "\n",
    "        # Model predictions as deterministic functions of weights and individual model predictions\n",
    "        model_prediction = pm.Deterministic('prediction', weights[0] * probs_rfc + \n",
    "                                                        weights[1] * probs_xgb + \n",
    "                                                        weights[2] * probs_lgr + weights[3] * probs_svm )\n",
    "\n",
    "        # Likelihood (sampling distribution) of observations\n",
    "        observed = pm.Bernoulli('obs', p=model_prediction, observed=y_test)\n",
    "\n",
    "        # Posterior distribution\n",
    "        trace = pm.sample(2000, return_inferencedata=True)\n",
    "\n",
    "    with model:\n",
    "        ppc = pm.sample_posterior_predictive(trace)\n",
    "\n",
    "    # # Obtain the mean prediction from the posterior predictive distribution\n",
    "    mean_prediction = np.mean(ppc['posterior_predictive']['obs'], axis=0)\n",
    "    final_predictions = (mean_prediction > 0.5).astype(int)\n",
    "\n",
    "    mp = np.mean(mean_prediction, axis=0)\n",
    "    final_predictions = (mp > 0.5).astype(int)\n",
    "\n",
    "    print(\"Bayesian Ensemble Accuracy:\", accuracy_score(y_test, final_predictions))\n",
    "    print(\"Bayesian Ensemble F1 Score:\", f1_score(y_test, final_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(x_train, y_train, x_test, y_test):\n",
    "    # Initialize the LinearSVC model\n",
    "    svm = LinearSVC(random_state=1, max_iter=10000)  # You can adjust max_iter based on convergence requirements\n",
    "\n",
    "    # Since SVMs are sensitive to the scaling of the data, it's often good practice to scale the features\n",
    "    # especially for high-dimensional data. We use a pipeline to combine scaling and the classifier.\n",
    "    pipeline = make_pipeline(StandardScaler(), CalibratedClassifierCV(svm))\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    pipeline.fit(x_train, y_train)\n",
    "\n",
    "    # Predict probability estimates for the test set\n",
    "    y_pre_proba_svm = pipeline.predict_proba(x_test)  # Probability of the positive class\n",
    "\n",
    "    y_pre_svm=np.where(pd.Series(y_pre_proba_svm[:,1])>0.5,1,0)\n",
    "    f1_svm = f1_score(y_pre_svm,y_test)\n",
    "    print(\"Test F1-score:\",round(f1_svm,3))\n",
    "    \n",
    "    return f1_svm, y_pre_proba_svm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack(X_train, y_train, X_test, y_test, lgr, xgb, rfc):\n",
    "    stack = StackingClassifier(\n",
    "        estimators=[('lgr', lgr), ('xgb', xgb), ('rfc', rfc)],\n",
    "        final_estimator=LogisticRegression(),\n",
    "        passthrough=True,\n",
    "        cv=5\n",
    "    )\n",
    "\n",
    "    stack.fit(X_train, y_train)\n",
    "    # Predict and evaluate the model\n",
    "    # y_pred = stack.predict(X_test)\n",
    "    y_pred_proba = stack.predict_proba(X_test)\n",
    "    \n",
    "    y_pred = np.where(pd.Series(y_pred_proba[:,1])>0.5,1,0)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1_stack = f1_score(y_test, y_pred)\n",
    "    print(f\"Stacking Model Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"Stacking Model F1: {f1_stack:.3f}\")\n",
    "    return f1_stack, y_pred_proba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_process():\n",
    "    # an attempt at the gaussian process "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
